1. 数据清洗

   1. 缺失值处理

      - 删除记录
      - 数据插补

      - 不处理？？？

      拉格朗日插值法

      平面上已知n个点，可以找到一个n-1次多项式
      $$
      y=a_0+a_1x+a_2x^2+...+a_kx^k
      $$

      $$
      k = n-1
      $$

      可解得
      $$
      L（x)=\sum_{i=0}^ny_i\prod_{j=0,j\neq1}^n\frac{x-x_j}{x_i-x_j}
      $$
      将缺失的函数值对应的点x带入插值多项式得到缺失值的近似值L(x)。

      -----------------

      拉格朗日插值法实例

      ```python
      # -*- coding: utf-8 -*-
      
      import pandas as pd  # 导入数据分析库Pandas
      from scipy.interpolate import lagrange  # 导入拉格朗日插值函数
      
      inputfile = '../data/catering_sale.xls'  # 销量数据路径
      outputfile = '../tmp/sales.xls'  # 输出数据路径
      
      data = pd.read_excel(inputfile)  # 读入数据
      data[u'销量'][(data[u'销量'] < 400) | (data[u'销量'] > 5000)] = None  # 过滤异常值，将其变为空值
      
      # 自定义列向量插值函数
      # s为列向量，n为被插值的位置，k为取前后的数据个数，默认为2
      def ployinterp_column(s, n, k=2):
        y = s.iloc[list(range(n-k, n)) + list(range(n+1, n+1+k))]  # 取数
        y = y[y.notnull()]  # 剔除空值
        return lagrange(y.index, list(y))(n)  # 插值并返回插值结果
      
      # 逐个元素判断是否需要插值
      for i in data.columns:
        for j in range(len(data)):
          if (data[i].isnull())[j]:  # 如果为空即插值。
            data.loc[j,i] = ployinterp_column(data[i], j)
      
      data.to_excel(outputfile)  # 输出结果，写入文件
      ```

      对表中缺失值进行插补

   2. 异常值处理

2. 数据集成

   1. 实体识别
   2. 冗余属性识别

3. 数据变换

   1. 简单函数变换

   2. 规范化

      ```python
      import pandas as pd
      datafile = '../data/normalization_data.xls' # 参数初始化
      data = pd.read_excel(datafile, header = None) # 读取数据
      
      (data - data.min())/(data.max() - data.min()) # 最小-最大规范化
      (data - data.mean())/data.std() # 零-均值规范化
      data/10**np.ceil(np.log10(data.abs().max())) # 小数定标规范化
      ```

      运行结果

      ```
      >>> data # 原数据集
           0    1    2     3
      0   78  521  602  2863
      1  144 -600 -521  2245
      2   95 -457  468 -1283
      3   69  596  695  1054
      4  190  527  691  2051
      5  101  403  470  2487
      6  146  413  435  2571
      >>> (data - data.min())/(data.max() - data.min()) # 最小-最大规范化
                0         1         2         3
      0  0.074380  0.937291  0.923520  1.000000
      1  0.619835  0.000000  0.000000  0.850941
      2  0.214876  0.119565  0.813322  0.000000
      3  0.000000  1.000000  1.000000  0.563676
      4  1.000000  0.942308  0.996711  0.804149
      5  0.264463  0.838629  0.814967  0.909310
      6  0.636364  0.846990  0.786184  0.929571
      >>> (data - data.mean())/data.std() # 零-均值规范化
                0         1         2         3
      0 -0.905383  0.635863  0.464531  0.798149
      1  0.604678 -1.587675 -2.193167  0.369390
      2 -0.516428 -1.304030  0.147406 -2.078279
      3 -1.111301  0.784628  0.684625 -0.456906
      4  1.657146  0.647765  0.675159  0.234796
      5 -0.379150  0.401807  0.152139  0.537286
      6  0.650438  0.421642  0.069308  0.595564
      >>> data/10**np.ceil(np.log10(data.abs().max())) # 小数定标规范化
             0      1      2       3
      0  0.078  0.521  0.602  0.2863
      1  0.144 -0.600 -0.521  0.2245
      2  0.095 -0.457  0.468 -0.1283
      3  0.069  0.596  0.695  0.1054
      4  0.190  0.527  0.691  0.2051
      5  0.101  0.403  0.470  0.2487
      6  0.146  0.413  0.435  0.2571
      ```

   3. 连续属性离散化

      ```python
      # -*- coding: utf-8 -*-
      
      # 代码4-3  数据离散化
      import pandas as pd
      import numpy as np
      datafile = '../data/discretization_data.xls'  # 参数初始化
      data = pd.read_excel(datafile)  # 读取数据
      data = data[u'肝气郁结证型系数'].copy()
      k = 4
      
      d1 = pd.cut(data, k, labels = range(k))  # 等宽离散化，将data数据集平均按照4份进行离散，各个类比依次命名为0,1,2,3
      
      #等频率离散化
      w = [1.0*i/k for i in range(k+1)]
      w = data.describe(percentiles = w)[4:4+k+1]  # 使用describe函数自动计算分位数
      w[0] = w[0]*(1-1e-10) # 为了包含数据集的最小值，等频初始值下调
      d2 = pd.cut(data, w, labels = range(k)) # data数据集按照(w_0,w_1]的方式进行离散
      
      from sklearn.cluster import KMeans  # 引入KMeans
      kmodel = KMeans(n_clusters = k, n_jobs = 4)  # 建立模型，n_jobs是并行数，一般等于CPU数较好
      kmodel.fit(np.array(data).reshape((len(data), 1)))  # 训练模型
      c = pd.DataFrame(kmodel.cluster_centers_).sort_values(0)  # 输出聚类中心，并且排序（默认是随机序的）
      w = c.rolling(2).mean()  # 相邻两项求中点，作为边界点
      w = w.dropna() # 剔除空值
      w = [0] + list(w[0]) + [data.max()]  # 把首末边界点加上，将w转换为列表，在列表头加上边界值 0，在列表末尾加上边界值data.max()
      d3 = pd.cut(data, w, labels = range(k)) # 按照w的边界点进行离散
      def cluster_plot(d, k):  # 自定义作图函数来显示聚类结果
        import matplotlib.pyplot as plt
        plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
        plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号
        
        plt.figure(figsize = (8, 3))
        for j in range(0, k):
          plt.plot(data[d==j], [j for i in d[d==j]], 'o') # 横坐标筛选数据集中聚类类型j类型的数据，绘制在高度j上，使用o标记
        
        plt.ylim(-0.5, k-0.5) # 设置y轴范围
        return plt
      
      cluster_plot(d1, k).show()
      cluster_plot(d2, k).show()
      cluster_plot(d3, k).show()
      ```

      运行结果

      等宽离散化

      <img src="https://github.com/HawZZ/py_dand_commit/blob/master/Chapter%204/等宽离散化.png" alt="等宽离散化" style="zoom:36%;" />

      等频离散化

      <img src="https://github.com/HawZZ/py_dand_commit/blob/master/Chapter%204/等频离散化.png" alt="等频离散化" style="zoom:36%;" />

      （一维）聚类离散化

      <img src="https://github.com/HawZZ/py_dand_commit/blob/master/Chapter%204/聚类离散化.png" alt="聚类离散化" style="zoom:36%;" />

      对数据离散化后，如上述结果将数据分成4类，然后将每一类记为同一个标识，再进行建模

   4. 属性构造

      ```python
      # -*- coding: utf-8 -*-
      
      # 代码4-4 线损率属性构造
      import pandas as pd
      
      # 参数初始化
      inputfile= '../data/electricity_data.xls'  # 供入供出电量数据
      outputfile = '../tmp/electricity_data.xls'  # 属性构造后数据文件
      
      data = pd.read_excel(inputfile)  # 读入数据
      data[u'线损率'] = (data[u'供入电量'] - data[u'供出电量']) / data[u'供入电量']
      
      data.to_excel(outputfile, index = False)  # 保存结果
      ```

      运行结果

      ```
         供入电量  供出电量       线损率
      0   986   912  0.075051
      1  1208  1083  0.103477
      2  1108   975  0.120036
      3  1082   934  0.136784
      4  1285  1102  0.142412
      ```

   5. 小波变换
   
      ```python
      # -*- coding: utf-8 -*-
      
      # 代码4-5  小波变换特征提取代码
      # 利用小波分析进行特征分析
      # 参数初始化
      inputfile= '../data/leleccum.mat'  # 提取自Matlab的信号文件
      
      from scipy.io import loadmat  # mat是Python专用格式，需要用loadmat读取它
      mat = loadmat(inputfile)
      signal = mat['leleccum'][0]
      
      import pywt  # 导入PyWavelets
      coeffs = pywt.wavedec(signal, 'bior3.7', level = 5)
      # 返回结果为level+1个数字，第一个数组为逼近系数数组，后面的依次是细节系数数组
      ```
   
4. 数据规约

   1. 属性规约
      主成分分析降维

      ```python
      # -*- coding: utf-8 -*-
      
      # 代码4-6 主成分分析降维
      import pandas as pd
      
      # 参数初始化
      inputfile = '../data/principal_component.xls'
      outputfile = '../tmp/dimention_reducted.xls'  # 降维后的数据
      
      data = pd.read_excel(inputfile, header = None)  # 读入数据
      
      from sklearn.decomposition import PCA
      
      pca = PCA()
      pca.fit(data)
      
      pca.components_  # 返回模型的各个特征向量
      
      
      pca.explained_variance_ratio_  # 返回各个成分各自的方差百分比
      
      
      
      
      # 代码4-7 计算成分结果
      pca = PCA(3)
      pca.fit(data)
      low_d = pca.transform(data)  # 用它来降低维度
      pd.DataFrame(low_d).to_excel(outputfile)  # 保存结果
      pca.inverse_transform(low_d)  # 必要时可以用inverse_transform()函数来复原数据
      
      low_d
      ```

      运行结果

      pca.components_  # 返回模型的各个特征向量

      ```
      array([[ 0.56788461,  0.2280431 ,  0.23281436,  0.22427336,  0.3358618 ,
               0.43679539,  0.03861081,  0.46466998],
             [ 0.64801531,  0.24732373, -0.17085432, -0.2089819 , -0.36050922,
              -0.55908747,  0.00186891,  0.05910423],
             [-0.45139763,  0.23802089, -0.17685792, -0.11843804, -0.05173347,
              -0.20091919, -0.00124421,  0.80699041],
             [-0.19404741,  0.9021939 , -0.00730164, -0.01424541,  0.03106289,
               0.12563004,  0.11152105, -0.3448924 ],
             [-0.06133747, -0.03383817,  0.12652433,  0.64325682, -0.3896425 ,
              -0.10681901,  0.63233277,  0.04720838],
             [ 0.02579655, -0.06678747,  0.12816343, -0.57023937, -0.52642373,
               0.52280144,  0.31167833,  0.0754221 ],
             [-0.03800378,  0.09520111,  0.15593386,  0.34300352, -0.56640021,
               0.18985251, -0.69902952,  0.04505823],
             [-0.10147399,  0.03937889,  0.91023327, -0.18760016,  0.06193777,
              -0.34598258, -0.02090066,  0.02137393]])
      ```

      pca.explained_variance_ratio_  # 返回各个成分各自的方差百分比（贡献率）

      ```
      array([7.74011263e-01, 1.56949443e-01, 4.27594216e-02, 2.40659228e-02,
             1.50278048e-03, 4.10990447e-04, 2.07718405e-04, 9.24594471e-05])
      ```

      上述结果为data数据的7个特征根对应的7个单位特征向量，以及各个成分各自的方差百分比（即贡献率）。方差百分比越大，说明向量的权重越大。

      当选取前三个主成分时，累计贡献率已经达到97.37%（>95%，有统计学意义），说明选取前三个主成分进行计算就可以了，因此重新建立PCA模型，设置n_components = 3，计算出成分结果。

      降维结果 low_d

      ```
      array([[  8.19133694,  16.90402785,   3.90991029],
             [  0.28527403,  -6.48074989,  -4.62870368],
             [-23.70739074,  -2.85245701,  -0.4965231 ],
             [-14.43202637,   2.29917325,  -1.50272151],
             [  5.4304568 ,  10.00704077,   9.52086923],
             [ 24.15955898,  -9.36428589,   0.72657857],
             [ -3.66134607,  -7.60198615,  -2.36439873],
             [ 13.96761214,  13.89123979,  -6.44917778],
             [ 40.88093588, -13.25685287,   4.16539368],
             [ -1.74887665,  -4.23112299,  -0.58980995],
             [-21.94321959,  -2.36645883,   1.33203832],
             [-36.70868069,  -6.00536554,   3.97183515],
             [  3.28750663,   4.86380886,   1.00424688],
             [  5.99885871,   4.19398863,  -8.59953736]])
      ```

      原始数据从8维降到3维，且该三维数据占了原始数据95%以上的信息。

   2. 数值规约

5. 主要数据预处理函数

   | 函数名      | 函数功能                                         | 所属库       |
   | ----------- | ------------------------------------------------ | ------------ |
   | interpolate | 一维、高维数据插值                               | scipy        |
   | unique      | 去除数据中的重复元素，得到单质元素列表，对象方法 | pandas/numpy |
   | isnull      | 判断是否空值                                     | pandas       |
   | notnull     | 判断是否非空值                                   | pandas       |
   | PCA         | 对指标变量矩阵进行主成分分析                     | scikit-learn |
   | random      | 生成随机矩阵                                     | numpy        |

   interpolate 使用格式 

   ```python
   from scipy.interpolate import lagrange
   f = lagrange(x,y)
   ```

   unique方法使用格式

   ```python
   np.unique(D) # D是一维数据，可以使list、array、Series
   D.unique() # D是pandas的Series对象
   ```

   isnull/notnull使用格式

   ```python
   D.isnull()
   D.notnull()
   # D要求是Series对象，返回一个布尔Series
   D[D.isnull()]
   D[D.notnull()]
   # 可以找出D中的空值/非空值
   ```

   random使用格式

   ```python
   np.random.rand(k,m,n,...) # 生成一个k*m*n*...随机矩阵，其元素均匀分布在区间(0,1)上
   np.random.randn(k,m,n,...) # 生成一个k*m*n*...随机矩阵，其元素服从标准正态分布
   ```

   PCA使用格式

   ```python
   from sklearn.decomposition import PCA
   model = PCA()
   ```

   PCA是建模函数，然后需要用model.fit(D)，D为需要进行主成分分析的数据矩阵。训练结束后可以通过model.components\_获取特征向量，以及model.explained\_variance\_ratio获取各个属性的贡献率。

   通过贡献率整理主要的特征向量（累计贡献率>95%），使用PCA函数选定主要成分进行降维

   ```
   model = PCA(n) # 保留主要的n个特征向量
   model.fit(D)
   low_b = model.transform(D)
   ```

   

